# 深度学习

## 数据操作

![image-20250412230953960](C:\Users\zxz2803\AppData\Roaming\Typora\typora-user-images\image-20250412230953960.png)

### 一个元素

获取矩阵中单个元素的索引方式为 `[行索引, 列索引]`

例、[1,2]表示行索引为1，列索引为2

### 一行

获取矩阵中某一行的方式是 `[行索引, :]`

其中 `:` 表示选取该行所有列的元素

### 一列

获取矩阵中某一列的方式是 `[:, 列索引]`

### 子区域

在 Python 的切片语法中，`start:stop` 表示从 `start` 索引（包含 ）到 `stop` 索引（不包含 ）的范围 。

例、`[1:3, 1:]` 表示选取矩阵中第 `1` 行（包含）到第 `3` 行（不包含 ），且从第 `1` 列（包含 ）开始到最后一列的子区域，对应元素为 ``[6, 7, 8, 10, 11, 12]`

`::` 后面跟的数字表示步长

例、`[::3, ::2]` 表示在行方向上，从第 `0` 行开始，每隔 `3` 行选取一行；在列方向上，从第 `0` 列开始，每隔 `2` 列选取一列 。最终选取的元素为 `[1, 3, 13, 15]`

## 线性回归



- 张量是一个多维数组，它可以具有任意数量的维度。
- 零维张量是一个标量（Scalar），例如一个单独的数字；
- 一维张量是一个向量（Vector），可以表示为一列或一行数字；
- 二维张量是一个矩阵（Matrix），由行和列组成；
- 三维及以上的张量则具有更复杂的结构，用于表示更高维度的数据。

`torch.normal`用于从正态分布（也称为高斯分布）中抽取随机样本，从而创建张量。

`torch.matmul`用于执行张量之间的矩阵乘法。

梯度下降公式

![image-20250417200047541](C:\Users\zxz2803\AppData\Roaming\Typora\typora-user-images\image-20250417200047541.png)

标量关于向量求导，得到的是一个向量；

向量关于向量求导；得到的是一个矩阵

`# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值
x.grad.zero_() #原地操作
y = x.sum() # 相当于向量x乘一个单位向量 y=x1+x2+x3+x4...
y.backward() #求梯度自然为1 y是一个标量，x是一个向量，标量关于向量求导，得到的是一个向量。
x.grad`
